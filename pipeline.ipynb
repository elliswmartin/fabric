{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base data sci libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# image processing libraries\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "# sklearn for pipeline creation and grid-search cv\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# BELOW ONLY NEEDED IF WE GO WITH THE IMPLEMENTATION OF A CNN, BUT I THINK THAT WOULD BE OVER THE TOP FOR THIS TASK\n",
    "# ! pip install tensorflow\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models\n",
    "\n",
    "np.random.seed(23)\n",
    "plt.rcParams['figure.figsize'] = (4, 4)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To-Dos\n",
    "* Add feature functions into cell with extract_log_features() mimicking the format and the instructions there\n",
    "* Decide on strategy in terms of flattening images or using scalar features -- what is worth spending computational umph on based on data exploration?\n",
    "* Update parse_data() to include any features generated from the functions defined in the featurize block\n",
    "* Add PCA and any other data exploration we want to complete for feature selection\n",
    "* Once we have features, tweak and play with the grid-search CV to get some baseline results for our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURIZE\n",
    "\n",
    "We need to translate the features we have already created + those in the proposal + those in the dataset paper into features that are either scalars to be appended to the image vector or a vector of features that can be concatenated onto the end of the feature vector.\n",
    "\n",
    "Below I have created a formula which defaults to producing a scalar representation of the Laplacian of Gaussian filter. You could also run the filter across the image and then flatten the resulting LoG filtered matrix into a vector (more computationally intensive). This function can be used as a template for the development of other feature formulae to be included in the df creation in the parse_data() function.\n",
    "\n",
    "##### ERIN TO DO - remember how to use pandas pipeline so we can pass these functions as arguments into parse_data() instead of entering them manually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_log_features(image_array, sigma=0.7, scalar=True):\n",
    "    \"\"\"\n",
    "    Extract Laplacian of Gaussian (LoG) features from an image array.\n",
    "    \n",
    "    Parameters:\n",
    "        image_array (numpy.ndarray): The input image array.\n",
    "        sigma (float): The sigma value for the Gaussian filter. Controls the amount of smoothing.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The LoG filtered image as a feature vector.\n",
    "    \"\"\"\n",
    "    # apply Laplacian of Gaussian filter\n",
    "    log_image = ndimage.gaussian_laplace(image_array, sigma=sigma)\n",
    "\n",
    "    if scalar:\n",
    "        \n",
    "        # OPTION 1\n",
    "        \n",
    "        # feature scalar: the sum of absolute values in the LoG image (a simple measure of edginess)\n",
    "        feature_scalar = np.sum(np.abs(log_image))\n",
    "\n",
    "        return feature_scalar\n",
    "\n",
    "    else:\n",
    "        # OPTION 2\n",
    "        \n",
    "        # feature vector: flatten the LoG image to use as a feature vector directly\n",
    "        feature_vector = log_image.flatten()\n",
    "\n",
    "        return feature_vector\n",
    "    \n",
    "### TO DO - ISI & ELLIS - Immitate the above formula to either create scalar features or feature vectors from those formulae we already explored ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence, each image needs to be represented as a feature vector. In the D200 example, the photos of the clothing items were in grayscale and then normalized and then reshaped using `train_images_vectors = np.reshape(train_images, (len(train_images), -1))` where train images is an array of n 28 x 28 training images (i.e. train_images.shape = (n, 28, 28)). After each image is reshaped, the vector is 1 x 784 (i.e. 28 x 28).\n",
    "\n",
    "We need a dataframe at the end of the day, where each row represents an image and its features. I want to talk more about which features we are actually going to leverage and use as a team, but for right now, I will build the plumbing to be able to run a classification once we actually have the features using the 'full image' technique employed in D200. In our case the images are still 200 x 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Data to DF + Add In Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(folder_path):\n",
    "    image_vectors = []  # image data\n",
    "    labels = []  # labels\n",
    "    ids = []  # unique IDs\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".png\"):\n",
    "            parts = filename.split('_')\n",
    "            fabType = parts[0]\n",
    "            id1 = parts[1]\n",
    "            id2 = parts[3].split('.')[0]  # Remove .png extension\n",
    "            \n",
    "            unique_id = id1 + id2\n",
    "\n",
    "            img = Image.open(os.path.join(folder_path, filename)).convert('L')\n",
    "            img_array = np.array(img)\n",
    "            \n",
    "            # normalize the image vector to be between 0 and 1\n",
    "            img_vector_normalized = img_array.flatten() / 255.0\n",
    "\n",
    "            scalar_features = []\n",
    "            scalar_features.append(extract_log_features(img_array))\n",
    "\n",
    "            ### APPEND ANY OTHER SCALAR FEATURES ###\n",
    "\n",
    "            scalar_features_array = np.array(scalar_features)\n",
    "            # new_vector_with_scalar = np.append(img_vector_normalized, log_scalar)\n",
    "\n",
    "            log_vector = extract_log_features(img_array, scalar=False)\n",
    "            \n",
    "            ### APPEND ANY OTHER VECTORIZED FEATURES ###\n",
    "\n",
    "            final_img_feature_vector  = np.concatenate((img_vector_normalized, scalar_features_array, log_vector)) # BE SURE TO ADD ANY FEATURE VECTORS HERE\n",
    "\n",
    "            image_vectors.append(img_vector_normalized)\n",
    "            labels.append(fabType)\n",
    "            ids.append(unique_id)\n",
    "\n",
    "    X = np.array(image_vectors)\n",
    "    Y = np.array(labels)\n",
    "    unique_ids = np.array(ids)\n",
    "    return X, Y, unique_ids   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - the cell below takes ~25 seconds to run on my mac. It may take a bit longer depending on your processing power and memory. If this becomes an issue, we can save the dataframe it creates as a pickle file, which could be saved to sub-samples and you two would be able to use without issue i.e. my computer processes and adds features and then saves it as a compressed python binary to allow for easy access that avoids double processing later. Just let me know if we need to do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>39993</th>\n",
       "      <th>39994</th>\n",
       "      <th>39995</th>\n",
       "      <th>39996</th>\n",
       "      <th>39997</th>\n",
       "      <th>39998</th>\n",
       "      <th>39999</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "      <th>uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.086275</td>\n",
       "      <td>0.101961</td>\n",
       "      <td>0.101961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>Blended</td>\n",
       "      <td>0</td>\n",
       "      <td>8821c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.439216</td>\n",
       "      <td>0.478431</td>\n",
       "      <td>0.572549</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.749020</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.760784</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>Denim</td>\n",
       "      <td>1</td>\n",
       "      <td>1503c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>0.298039</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168627</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.125490</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.160784</td>\n",
       "      <td>0.192157</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>Polyester</td>\n",
       "      <td>2</td>\n",
       "      <td>16132c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.380392</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.356863</td>\n",
       "      <td>0.345098</td>\n",
       "      <td>0.341176</td>\n",
       "      <td>0.345098</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.423529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.415686</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.545098</td>\n",
       "      <td>Blended</td>\n",
       "      <td>0</td>\n",
       "      <td>3621d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.298039</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.298039</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.278431</td>\n",
       "      <td>0.345098</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>3</td>\n",
       "      <td>2333a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40003 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.062745  0.058824  0.066667  0.062745  0.062745  0.066667  0.082353   \n",
       "1  0.266667  0.258824  0.270588  0.301961  0.313725  0.352941  0.439216   \n",
       "2  0.235294  0.247059  0.274510  0.298039  0.309804  0.305882  0.282353   \n",
       "3  0.380392  0.364706  0.356863  0.345098  0.341176  0.345098  0.388235   \n",
       "4  0.321569  0.317647  0.305882  0.298039  0.294118  0.301961  0.309804   \n",
       "\n",
       "          7         8         9  ...     39993     39994     39995     39996  \\\n",
       "0  0.086275  0.101961  0.101961  ...  0.066667  0.058824  0.054902  0.054902   \n",
       "1  0.478431  0.572549  0.607843  ...  0.200000  0.333333  0.749020  0.803922   \n",
       "2  0.274510  0.270588  0.258824  ...  0.168627  0.133333  0.125490  0.133333   \n",
       "3  0.435294  0.450980  0.423529  ...  0.400000  0.415686  0.450980  0.482353   \n",
       "4  0.313725  0.325490  0.333333  ...  0.376471  0.298039  0.247059  0.278431   \n",
       "\n",
       "      39997     39998     39999   category  label     uid  \n",
       "0  0.058824  0.062745  0.062745    Blended      0   8821c  \n",
       "1  0.788235  0.760784  0.717647      Denim      1   1503c  \n",
       "2  0.160784  0.192157  0.211765  Polyester      2  16132c  \n",
       "3  0.482353  0.505882  0.545098    Blended      0   3621d  \n",
       "4  0.345098  0.333333  0.309804     Cotton      3   2333a  \n",
       "\n",
       "[5 rows x 40003 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = './Subsamples/train'\n",
    "X, Y, unique_ids = parse_data(folder_path)\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df['category'] = pd.Categorical(Y)\n",
    "df['label'], _ = pd.factorize(df['category'])\n",
    "df['uid'] = unique_ids\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this dataframe, which is in the same format as the data shown in the MNIST D200 homework, it should be easy to set up t-SNE and PCA. HOWEVER - at this point we don't have any true features, as our existing features need to be converted to columns in this dataframe. Below is skeleton code that can be filled out to run and add these features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO - INSERT PCA AND OTHER EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - Multi-Class Classifier\n",
    "\n",
    "\n",
    "The function below takes in a test DF, features (which we can modified in the event we have thousands of columns and they are named numerically -- in this case we just need to drop the target variable to define X). It runs cross validation to determine the best hyperparameters to be used for the support vector machine model, returning a grid search object with the following attributes (per ChatGPT):\n",
    "\n",
    "<b>Attributes</b>\n",
    "* best_estimator_: The estimator that was chosen by the search, i.e., the estimator which gave highest score (or smallest loss if specified) on the left out data. Not available if refit=False.\n",
    "* best_score_: The score of the best_estimator on the left out data.\n",
    "* best_params_: The parameter setting that gave the best results on the hold out data.\n",
    "* best_index_: The index (of the cv_results_ arrays) which corresponds to the best candidate parameter setting.\n",
    "* cv_results_: A dictionary with keys as column headers and values as columns, that can be imported into a pandas DataFrame. This attribute provides scores, fit times, score times, and parameters for all the candidate models. It contains a lot of detailed information for each parameter combination that was evaluated.\n",
    "* scorer_: The function or a dictionary of functions that scores the predictions on the test set.\n",
    "* n_splits_: The number of cross-validation splits (folds/iterations).\n",
    "* refit_time_: Time for refitting the best estimator on the whole dataset (available only if refit is set to True).\n",
    "\n",
    "<b>Methods</b>\n",
    "* fit(X, y=None, groups=None): Run fit with all sets of parameters.\n",
    "* predict(X): Call predict on the estimator with the best found parameters.\n",
    "* score(X, y=None): Returns the score on the given data, if the estimator has been refit.\n",
    "* predict_proba(X): Call predict_proba on the estimator with the best found parameters, if available.\n",
    "* decision_function(X): Call decision_function on the estimator with the best found parameters, if available.\n",
    "* transform(X): Call transform on the estimator with the best found parameters, if available.\n",
    "* inverse_transform(X): Call inverse_transform on the estimator with the best found parameters, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_grid_search_cv(dataframe, features, target, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Perform grid search cross-validation for SVM classifier on the given dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe: The pandas DataFrame containing the dataset.\n",
    "    - features: List of column names to be used as features.\n",
    "    - target: The name of the column to be used as the target variable.\n",
    "    - cv_folds: Number of folds for cross-validation.\n",
    "    \n",
    "    Returns:\n",
    "    - grid_search: The fitted GridSearchCV object.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate the features and the target variable\n",
    "    X = dataframe[features]\n",
    "    y = dataframe[target]\n",
    "    \n",
    "    # Split the data into training and testing sets (optional, could also perform CV on the entire dataset)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Define a pipeline that includes scaling and the classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Feature scaling is important for SVM\n",
    "        ('svm', SVC(probability=True))  # SVM classifier\n",
    "    ])\n",
    "    \n",
    "    # Parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'svm__C': [0.1, 1, 10],  # SVM regularization parameter\n",
    "        'svm__kernel': ['linear', 'rbf'],  # Kernel type to be used in the algorithm\n",
    "        'svm__gamma': ['scale', 'auto']  # Kernel coefficient for 'rbf', 'poly' and 'sigmoid'\n",
    "    }\n",
    "    \n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=cv_folds, scoring='accuracy', verbose=1)\n",
    "    \n",
    "    # Perform grid search cross-validation\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best parameters found: \", grid_search.best_params_)\n",
    "    print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "    \n",
    "    # Optionally, evaluate on the test set\n",
    "    test_score = grid_search.score(X_test, y_test)\n",
    "    print(\"Test set score: {:.2f}\".format(test_score))\n",
    "    \n",
    "    return grid_search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "The below code has not been tested or debugged - it was read through against a TDS article for accuracy, but it would need significant work, change in approach (and potentially additional compute) if we are to make it work. I wanted to store it so I don't lose the information though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple cnn model\n",
    "# def create_cnn_model(input_shape, num_classes):\n",
    "#     model = models.Sequential([\n",
    "#         # convolutional layer with ReLU activation and Max Pooling\n",
    "#         layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "#         layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "#         # convolutional layer 2\n",
    "#         layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#         layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "#         # flatten the output and add dense layers for classification\n",
    "#         layers.Flatten(),\n",
    "#         layers.Dense(64, activation='relu'),\n",
    "#         layers.Dense(num_classes, activation='softmax')\n",
    "#     ])\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# input_shape = (64, 64, 3)  # input shape (height, width, channels)\n",
    "# num_classes = 5\n",
    "\n",
    "# model = create_cnn_model(input_shape, num_classes)\n",
    "\n",
    "# # compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# # train_images, train_labels = ... \n",
    "# # val_images, val_labels = ... \n",
    "\n",
    "# train model\n",
    "# history = model.fit(train_images, train_labels, epochs=10, validation_data=(val_images, val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO - ADD SVM CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tanlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
