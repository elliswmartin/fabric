{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Scalar Feature Dataset + Send to Pickle\n",
    "\n",
    "The following script runs and creates a scalar feature dataset for all of the training data and testing data using the feature functions defined in the feature_utils file. We used this approach as then we could take advantage of the strength of the strongest laptop on the team to run the processing and featurization. Once saved as a pickle file, which is a compressed binary representation of a pandas dataframe, it was very easy for other team members to import the pickle and run inference without having to process the data every time. This approach allowed us to parallelize efforts to explore models with confidence, as we knew we would be operating on the same processed training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base data sci libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# image processing libraries\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(23)\n",
    "plt.rcParams['figure.figsize'] = (4, 4)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "from utils.feature_utils import create_visual_vocab, extract_bovw_features, extract_hog_features, extract_wavelet_features, extract_log_features, extract_normals_features, extract_gabor_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below parser to produce the pickle file, which parses the raw image data and generates several arrays. These arrays are then turned into a DF and sent to a pickle file down below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Data to DF + Add In Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_all(folder_path, pretrained_kmeans=None):\n",
    "    \"\"\" \n",
    "    Run all feature functions and create df of all feature representations. Scalar or Vector \n",
    "    \n",
    "    Returns: \n",
    "        X (np.array): Feature values \n",
    "        Y (np.array): Categorical label for each image  \n",
    "        unique_ids (np.array): image IDs  \n",
    "    \"\"\"\n",
    "\n",
    "    image_vectors = []  # image data\n",
    "    labels = []  # labels\n",
    "    ids = []  # unique IDs\n",
    "\n",
    "    if pretrained_kmeans:\n",
    "        kmeans_fitted = pretrained_kmeans\n",
    "    else:\n",
    "        kmeans_fitted = create_visual_vocab(folder_path)\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".png\"):\n",
    "            parts = filename.split('_')\n",
    "            fabType = parts[0]\n",
    "            id1 = parts[1]\n",
    "            id2 = parts[3].split('.')[0]  # Remove .png extension\n",
    "            \n",
    "            unique_id = id1 + id2\n",
    "\n",
    "            image = Image.open(os.path.join(folder_path, filename)).convert('L')\n",
    "            img_array = np.array(image)\n",
    "\n",
    "            # sift expects a particular type of image so this needs to be done before \n",
    "            # the image is normalized for the other features. The output vector is between\n",
    "            # 0 and 1 and should not impact PCA\n",
    "            bovw_feature_vector = extract_bovw_features(img_array, kmeans_fitted)\n",
    "            \n",
    "            # normalize the image vector to be between 0 and 1 \n",
    "            img_array_std = (img_array - np.mean(img_array)) / np.std(img_array)\n",
    "\n",
    "            ### SCALAR FEATURES ###\n",
    "            hog_stats = extract_hog_features(img_array_std, 4, 20)\n",
    "            hog_mean, hog_sum, hog_var, hog_skew, hog_kurt = hog_stats[:5]\n",
    "            wavelet_stats = extract_wavelet_features(img_array_std)\n",
    "            wave_mean_cA, wave_var_cA, wave_mean_cD, wave_var_cD = wavelet_stats[:4]\n",
    "            # bovw_features = extract_bovw_features(img_array_std, 5, False)\n",
    "            # bovw_1, bovw_2, bovw_3, bovw_4, bovw_5 = bovw_features[:5] \n",
    "            \n",
    "            scalar_features = []\n",
    "            scalar_features.extend([\n",
    "                extract_log_features(img_array_std), \n",
    "                extract_normals_features(img_array_std), \n",
    "                extract_gabor_features(img_array_std),\n",
    "                hog_mean, hog_sum, hog_var, hog_skew, hog_kurt, \n",
    "                wave_mean_cA, wave_var_cA, wave_mean_cD, wave_var_cD, \n",
    "                *bovw_feature_vector])\n",
    "        \n",
    "            scalar_features_array = np.array(scalar_features)\n",
    "\n",
    "            image_vectors.append(scalar_features_array)\n",
    "            labels.append(fabType)\n",
    "            ids.append(unique_id)\n",
    "\n",
    "    X = np.array(image_vectors)\n",
    "    Y = np.array(labels)\n",
    "    unique_ids = np.array(ids)\n",
    "    return X, Y, unique_ids, kmeans_fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training dataframe pickled for easy future access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "      <th>uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3441.793288</td>\n",
       "      <td>2.028531e+07</td>\n",
       "      <td>1844.056770</td>\n",
       "      <td>0.150226</td>\n",
       "      <td>346.120911</td>\n",
       "      <td>0.005210</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>2.273737e-16</td>\n",
       "      <td>3.959789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>Blended</td>\n",
       "      <td>0</td>\n",
       "      <td>8821c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7211.992783</td>\n",
       "      <td>7.747671e+06</td>\n",
       "      <td>2271.840993</td>\n",
       "      <td>0.151475</td>\n",
       "      <td>348.998260</td>\n",
       "      <td>0.004833</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>2.046363e-16</td>\n",
       "      <td>3.864560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024324</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.021622</td>\n",
       "      <td>0.024324</td>\n",
       "      <td>0.043243</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>0.070270</td>\n",
       "      <td>Denim</td>\n",
       "      <td>1</td>\n",
       "      <td>1503c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8856.756862</td>\n",
       "      <td>5.854463e+06</td>\n",
       "      <td>1967.259618</td>\n",
       "      <td>0.160454</td>\n",
       "      <td>369.684998</td>\n",
       "      <td>0.002032</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>9.094947e-17</td>\n",
       "      <td>3.812086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>Polyester</td>\n",
       "      <td>2</td>\n",
       "      <td>16132c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7018.112788</td>\n",
       "      <td>7.817569e+06</td>\n",
       "      <td>1953.124972</td>\n",
       "      <td>0.152300</td>\n",
       "      <td>350.899200</td>\n",
       "      <td>0.004582</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>-6.821210e-17</td>\n",
       "      <td>3.879019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>0.022388</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.085821</td>\n",
       "      <td>0.074627</td>\n",
       "      <td>0.033582</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>Blended</td>\n",
       "      <td>0</td>\n",
       "      <td>3621d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7932.263905</td>\n",
       "      <td>6.971318e+06</td>\n",
       "      <td>2053.412469</td>\n",
       "      <td>0.157971</td>\n",
       "      <td>363.965454</td>\n",
       "      <td>0.002823</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-4.547474e-16</td>\n",
       "      <td>3.838197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.051020</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>0.044218</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.054422</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>3</td>\n",
       "      <td>2333a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0             1            2         3           4         5  \\\n",
       "0  3441.793288  2.028531e+07  1844.056770  0.150226  346.120911  0.005210   \n",
       "1  7211.992783  7.747671e+06  2271.840993  0.151475  348.998260  0.004833   \n",
       "2  8856.756862  5.854463e+06  1967.259618  0.160454  369.684998  0.002032   \n",
       "3  7018.112788  7.817569e+06  1953.124972  0.152300  350.899200  0.004582   \n",
       "4  7932.263905  6.971318e+06  2053.412469  0.157971  363.965454  0.002823   \n",
       "\n",
       "          6         7             8         9  ...        25        26  \\\n",
       "0  0.000093  0.000063  2.273737e-16  3.959789  ...  0.039062  0.031250   \n",
       "1 -0.000067  0.000038  2.046363e-16  3.864560  ...  0.024324  0.013514   \n",
       "2 -0.000002  0.000008  9.094947e-17  3.812086  ...  0.250000  0.129630   \n",
       "3  0.000011  0.000034 -6.821210e-17  3.879019  ...  0.007463  0.022388   \n",
       "4 -0.000003  0.000017 -4.547474e-16  3.838197  ...  0.047619  0.051020   \n",
       "\n",
       "         27        28        29        30        31   category  label     uid  \n",
       "0  0.027344  0.046875  0.062500  0.050781  0.050781    Blended      0   8821c  \n",
       "1  0.021622  0.024324  0.043243  0.016216  0.070270      Denim      1   1503c  \n",
       "2  0.194444  0.018519  0.004630  0.027778  0.004630  Polyester      2  16132c  \n",
       "3  0.044776  0.085821  0.074627  0.033582  0.029851    Blended      0   3621d  \n",
       "4  0.081633  0.013605  0.044218  0.047619  0.054422     Cotton      3   2333a  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Driver for PARSE ALL \n",
    "folder_path = './Subsamples/train'\n",
    "# X, Y, unique_ids, features_array = parse_all(folder_path)\n",
    "X, Y, unique_ids, kmeans_fitted_train = parse_all(folder_path)\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df['category'] = pd.Categorical(Y)\n",
    "df['label'], _ = pd.factorize(df['category'])\n",
    "df['uid'] = unique_ids\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¥’ PICKLEðŸš°ðŸ“ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pickle the dataframe\n",
    "train_df = df.copy()\n",
    "train_df.columns = train_df.columns.astype(str)\n",
    "PIXEL_COLS = train_df.columns.tolist()[:-3] # list of pixel header\n",
    "LABEL_COLS = ['label', 'category'] # list of labels header\n",
    "cols_reorder = LABEL_COLS + PIXEL_COLS\n",
    "train_df = train_df[cols_reorder]\n",
    "train_df.head()\n",
    "\n",
    "curr_date = '04dd' # Replace dd with date \n",
    "filename = f'./Subsamples/train_{curr_date}.pkl'\n",
    "train_df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the pickled df  \n",
    "filename = './pkls/train_0406.pkl'\n",
    "train_df = pd.read_pickle(filename)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test dataframe pickled for easy future access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "      <th>uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8401.258729</td>\n",
       "      <td>9.171677e+06</td>\n",
       "      <td>2193.809984</td>\n",
       "      <td>0.159283</td>\n",
       "      <td>366.988983</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1.080025e-16</td>\n",
       "      <td>3.784362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078704</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.023148</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>Polyester</td>\n",
       "      <td>2</td>\n",
       "      <td>6293d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11702.340470</td>\n",
       "      <td>4.712472e+06</td>\n",
       "      <td>2154.897678</td>\n",
       "      <td>0.161690</td>\n",
       "      <td>372.533752</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>1.284661e-15</td>\n",
       "      <td>3.697774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269565</td>\n",
       "      <td>0.079710</td>\n",
       "      <td>0.175362</td>\n",
       "      <td>0.024638</td>\n",
       "      <td>0.010145</td>\n",
       "      <td>0.044928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>3</td>\n",
       "      <td>1804b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7195.413672</td>\n",
       "      <td>1.209986e+07</td>\n",
       "      <td>1934.615054</td>\n",
       "      <td>0.151225</td>\n",
       "      <td>348.423218</td>\n",
       "      <td>0.004909</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>-2.046363e-16</td>\n",
       "      <td>3.843481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.006410</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.064103</td>\n",
       "      <td>0.089744</td>\n",
       "      <td>0.070513</td>\n",
       "      <td>0.070513</td>\n",
       "      <td>Blended</td>\n",
       "      <td>0</td>\n",
       "      <td>8933d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4753.699494</td>\n",
       "      <td>1.003820e+07</td>\n",
       "      <td>1746.407531</td>\n",
       "      <td>0.152653</td>\n",
       "      <td>351.713531</td>\n",
       "      <td>0.004475</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-1.136868e-16</td>\n",
       "      <td>3.930521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019108</td>\n",
       "      <td>0.019108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063694</td>\n",
       "      <td>0.050955</td>\n",
       "      <td>0.101911</td>\n",
       "      <td>0.108280</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>3</td>\n",
       "      <td>12133c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9354.039506</td>\n",
       "      <td>6.956389e+06</td>\n",
       "      <td>2133.356656</td>\n",
       "      <td>0.161588</td>\n",
       "      <td>372.299194</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>3.524292e-16</td>\n",
       "      <td>3.825473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>3</td>\n",
       "      <td>17161a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0             1            2         3           4         5  \\\n",
       "0   8401.258729  9.171677e+06  2193.809984  0.159283  366.988983  0.002407   \n",
       "1  11702.340470  4.712472e+06  2154.897678  0.161690  372.533752  0.001634   \n",
       "2   7195.413672  1.209986e+07  1934.615054  0.151225  348.423218  0.004909   \n",
       "3   4753.699494  1.003820e+07  1746.407531  0.152653  351.713531  0.004475   \n",
       "4   9354.039506  6.956389e+06  2133.356656  0.161588  372.299194  0.001667   \n",
       "\n",
       "          6         7             8         9  ...        25        26  \\\n",
       "0 -0.000030  0.000011  1.080025e-16  3.784362  ...  0.078704  0.083333   \n",
       "1 -0.000014  0.000005  1.284661e-15  3.697774  ...  0.269565  0.079710   \n",
       "2  0.000021  0.000043 -2.046363e-16  3.843481  ...  0.012821  0.006410   \n",
       "3  0.000101  0.000038 -1.136868e-16  3.930521  ...  0.019108  0.019108   \n",
       "4 -0.000027  0.000006  3.524292e-16  3.825473  ...  0.000000  0.000000   \n",
       "\n",
       "         27        28        29        30        31   category  label     uid  \n",
       "0  0.069444  0.074074  0.083333  0.023148  0.013889  Polyester      2   6293d  \n",
       "1  0.175362  0.024638  0.010145  0.044928  0.000000     Cotton      3   1804b  \n",
       "2  0.012821  0.064103  0.089744  0.070513  0.070513    Blended      0   8933d  \n",
       "3  0.000000  0.063694  0.050955  0.101911  0.108280     Cotton      3  12133c  \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000     Cotton      3  17161a  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Driver for PARSE ALL \n",
    "folder_path = './Subsamples/test'\n",
    "# X, Y, unique_ids, features_array = parse_all(folder_path)\n",
    "X, Y, unique_ids, _ = parse_all(folder_path, pretrained_kmeans=kmeans_fitted_train)\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df['category'] = pd.Categorical(Y)\n",
    "\n",
    "# mapping dictionary\n",
    "category_to_label = {'Blended': 0, 'Denim': 1, 'Polyester': 2, 'Cotton': 3, 'Wool': 4}\n",
    "\n",
    "# map the 'category' to 'label' using the dictionary defined by train_0406_scalar_non-aug.pkl\n",
    "df['label'] = df['category'].map(category_to_label)\n",
    "\n",
    "df['uid'] = unique_ids\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¥’ PICKLEðŸš°ðŸ“ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pickle the dataframe\n",
    "test_df = df.copy()\n",
    "test_df.columns = test_df.columns.astype(str)\n",
    "PIXEL_COLS = test_df.columns.tolist()[:-3] # list of pixel header\n",
    "LABEL_COLS = ['label', 'category'] # list of labels header\n",
    "cols_reorder = LABEL_COLS + PIXEL_COLS\n",
    "test_df = test_df[cols_reorder]\n",
    "test_df.head()\n",
    "\n",
    "curr_date = '0415' # Replace dd with date \n",
    "filename = f'./pkls/test_{curr_date}.pkl'\n",
    "test_df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4729, 34)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = './pkls/test_0415.pkl'\n",
    "train_df = pd.read_pickle(filename)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
