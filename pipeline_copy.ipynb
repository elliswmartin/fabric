{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base data sci libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "# image processing libraries\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "# sklearn for pipeline creation and grid-search cv\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#\n",
    "\n",
    "# BELOW ONLY NEEDED IF WE GO WITH THE IMPLEMENTATION OF A CNN, BUT I THINK THAT WOULD BE OVER THE TOP FOR THIS TASK\n",
    "# ! pip install tensorflow\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models\n",
    "\n",
    "np.random.seed(23)\n",
    "plt.rcParams['figure.figsize'] = (4, 4)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To-Dos\n",
    "* Add feature functions into cell with extract_log_features() mimicking the format and the instructions there\n",
    "* Decide on strategy in terms of flattening images or using scalar features -- what is worth spending computational umph on based on data exploration?\n",
    "* Update parse_data() to include any features generated from the functions defined in the featurize block\n",
    "* Add PCA and any other data exploration we want to complete for feature selection\n",
    "* Once we have features, tweak and play with the grid-search CV to get some baseline results for our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURIZE\n",
    "\n",
    "We need to translate the features we have already created + those in the proposal + those in the dataset paper into features that are either scalars to be appended to the image vector or a vector of features that can be concatenated onto the end of the feature vector.\n",
    "\n",
    "Below I have created a formula which defaults to producing a scalar representation of the Laplacian of Gaussian filter. You could also run the filter across the image and then flatten the resulting LoG filtered matrix into a vector (more computationally intensive). This function can be used as a template for the development of other feature formulae to be included in the df creation in the parse_data() function.\n",
    "\n",
    "##### ERIN TO DO - remember how to use pandas pipeline so we can pass these functions as arguments into parse_data() instead of entering them manually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_log_features(image_array, sigma=0.7, scalar=True):\n",
    "    \"\"\"\n",
    "    Extract Laplacian of Gaussian (LoG) features from an image array.\n",
    "    \n",
    "    Parameters:\n",
    "        image_array (numpy.ndarray): The input image array.\n",
    "        sigma (float): The sigma value for the Gaussian filter. Controls the amount of smoothing.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The LoG filtered image as a feature vector.\n",
    "    \"\"\"\n",
    "    # apply Laplacian of Gaussian filter\n",
    "    log_image = ndimage.gaussian_laplace(image_array, sigma=sigma)\n",
    "\n",
    "    if scalar:\n",
    "        \n",
    "        # OPTION 1\n",
    "        \n",
    "        # feature scalar: the sum of absolute values in the LoG image (a simple measure of edginess)\n",
    "        feature_scalar = np.sum(np.abs(log_image))\n",
    "\n",
    "        return feature_scalar\n",
    "\n",
    "    else:\n",
    "        # OPTION 2\n",
    "        \n",
    "        # feature vector: flatten the LoG image to use as a feature vector directly\n",
    "        feature_vector = log_image.flatten()\n",
    "\n",
    "        return feature_vector\n",
    "    \n",
    "### TO DO - ISI & ELLIS - Immitate the above formula to either create scalar features or feature vectors from those formulae we already explored ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(image_array, orientation, pixels, scalar=True):\n",
    "    \"\"\"\n",
    "    Extract HOG features from an image array.\n",
    "    \n",
    "    Parameters:\n",
    "        image_array (numpy.ndarray): The input image array.\n",
    "        saclar: if the output must be a scalar or a feature vector\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The HOG as a feature vector or scalar.\n",
    "    \"\"\"\n",
    "    #preprocessing\n",
    "    # convert to floating point image with intensity [0, 1]\n",
    "    if np.max(image_array) > 1:\n",
    "        img = image_array.astype(np.float32) / 255.0\n",
    "    # convert to grayscale\n",
    "    if len(img.shape) > 2:\n",
    "        img = rgb2gray(img)\n",
    "    gray_img = img\n",
    "    #HOG feature extraction\n",
    "    #the orientation and pixels will increase the detail (more orientations and less pixels are more computationally expensive)\n",
    "    feature_vector = hog(gray_img, orientations=orientation, pixels_per_cell=(pixels, pixels), visualize=False, feature_vector=True)  \n",
    "    if scalar:\n",
    "        feature_scalar = np.mean(feature_vector) #np.mean is for averaging features, np.sum is for the overall \"strength\" or \"intensity\" of the features.\n",
    "        return feature_scalar\n",
    "    else:\n",
    "        return feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_normals_features(image_array, scalar=True):\n",
    "    #preprocessing\n",
    "    # convert to floating point image with intensity [0, 1]\n",
    "    if np.max(image_array) > 1:\n",
    "        img = image_array.astype(np.float32) / 255.0\n",
    "    # convert to grayscale\n",
    "    if len(img.shape) > 2:\n",
    "        img = rgb2gray(img)\n",
    "    gray_img = img\n",
    "    # Compute gradients using Sobel operator\n",
    "    sobel_x = cv2.Sobel(gray_img, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobel_y = cv2.Sobel(gray_img, cv2.CV_64F, 0, 1, ksize=3)\n",
    "\n",
    "    # Compute normal vectors Nx, Ny, Nz\n",
    "    norm = np.sqrt(sobel_x**2 + sobel_y**2 + 1e-6)\n",
    "    nx = sobel_x / norm\n",
    "    ny = sobel_y / norm\n",
    "    nz = 1 / norm\n",
    "\n",
    "    # Concatenate nx, ny, nz along a new axis, and flatten it to form a 1D feature vector\n",
    "    feature_vector = np.stack((nx, ny, nz), axis=-1).reshape(-1)\n",
    "    if scalar:\n",
    "        feature_scalar = np.sum(feature_vector)\n",
    "        return feature_scalar\n",
    "    else:\n",
    "        return feature_vector\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence, each image needs to be represented as a feature vector. In the D200 example, the photos of the clothing items were in grayscale and then normalized and then reshaped using `train_images_vectors = np.reshape(train_images, (len(train_images), -1))` where train images is an array of n 28 x 28 training images (i.e. train_images.shape = (n, 28, 28)). After each image is reshaped, the vector is 1 x 784 (i.e. 28 x 28).\n",
    "\n",
    "We need a dataframe at the end of the day, where each row represents an image and its features. I want to talk more about which features we are actually going to leverage and use as a team, but for right now, I will build the plumbing to be able to run a classification once we actually have the features using the 'full image' technique employed in D200. In our case the images are still 200 x 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Data to DF + Add In Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(folder_path):\n",
    "    image_vectors = []  # image data\n",
    "    labels = []  # labels\n",
    "    ids = []  # unique IDs\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".png\"):\n",
    "            parts = filename.split('_')\n",
    "            fabType = parts[0]\n",
    "            id1 = parts[1]\n",
    "            id2 = parts[3].split('.')[0]  # Remove .png extension\n",
    "            \n",
    "            unique_id = id1 + id2\n",
    "\n",
    "            img = Image.open(os.path.join(folder_path, filename)).convert('L')\n",
    "            img_array = np.array(img)\n",
    "            \n",
    "            # normalize the image vector to be between 0 and 1\n",
    "            img_vector_normalized = img_array.flatten() / 255.0\n",
    "            \n",
    "            image_vectors.append(img_vector_normalized)\n",
    "            labels.append(fabType)\n",
    "            ids.append(unique_id)\n",
    "\n",
    "    X = np.array(image_vectors)\n",
    "    Y = np.array(labels)\n",
    "    unique_ids = np.array(ids)\n",
    "    return X, Y, unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_all(folder_path):\n",
    "    image_vectors = []  # image data\n",
    "    labels = []  # labels\n",
    "    ids = []  # unique IDs\n",
    "    features = []\n",
    "\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".png\"):\n",
    "            parts = filename.split('_')\n",
    "            fabType = parts[0]\n",
    "            id1 = parts[1]\n",
    "            id2 = parts[3].split('.')[0]  # Remove .png extension\n",
    "            \n",
    "            unique_id = id1 + id2\n",
    "\n",
    "            img = Image.open(os.path.join(folder_path, filename)).convert('L')\n",
    "            img_array = np.array(img)\n",
    "            \n",
    "            # normalize the image vector to be between 0 and 1\n",
    "            img_vector_normalized = img_array.flatten() / 255.0\n",
    "\n",
    "            ### APPEND ANY OTHER SCALAR FEATURES ###\n",
    "            scalar_features = []\n",
    "            scalar_features.append(extract_log_features(img_array))\n",
    "            scalar_features.append(extract_hog_features(img_array, 4, 20))\n",
    "            scalar_features.append(extract_normals_features(img_array))\n",
    "\n",
    "            \n",
    "\n",
    "            scalar_features_array = np.array(scalar_features)\n",
    "            # new_vector_with_scalar = np.append(img_vector_normalized, log_scalar)\n",
    "\n",
    "            ### APPEND ANY OTHER VECTORIZED FEATURES ###\n",
    "            log_vector = extract_log_features(img_array, scalar=False)\n",
    "            hog_vector = extract_hog_features(img_array, 4, 20, scalar=False)\n",
    "            normals_vector = extract_normals_features(img_array, scalar = False)\n",
    "            \n",
    "            \n",
    "            #NOTE: This is created but then not used, I added as an additional output (features)\n",
    "            final_img_feature_vector  = np.concatenate((img_vector_normalized, scalar_features_array, log_vector, hog_vector, normals_vector)) # BE SURE TO ADD ANY FEATURE VECTORS HERE\n",
    "            \n",
    "            image_vectors.append(img_vector_normalized)\n",
    "            labels.append(fabType)\n",
    "            ids.append(unique_id)\n",
    "            features.append(final_img_feature_vector)\n",
    "\n",
    "    X = np.array(image_vectors)\n",
    "    Y = np.array(labels)\n",
    "    unique_ids = np.array(ids)\n",
    "    features_array = np.array(features)\n",
    "    return X, Y, unique_id, features_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - the cell below takes ~25 seconds to run on my mac. It may take a bit longer depending on your processing power and memory. If this becomes an issue, we can save the dataframe it creates as a pickle file, which could be saved to sub-samples and you two would be able to use without issue i.e. my computer processes and adds features and then saves it as a compressed python binary to allow for easy access that avoids double processing later. Just let me know if we need to do that!\n",
    "\n",
    "Note 2 - After adding HOG and Normals features, it took 4 m 4.2 sec in my computer, the second time broke, so I created a code that is only to create the df. The original one + the 2 additional features is in parse_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "folder_path = './Subsamples/train'\n",
    "X, Y, unique_ids = parse_all(folder_path)\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df['category'] = pd.Categorical(Y)\n",
    "df['label'], _ = pd.factorize(df['category'])\n",
    "df['uid'] = unique_ids\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>39993</th>\n",
       "      <th>39994</th>\n",
       "      <th>39995</th>\n",
       "      <th>39996</th>\n",
       "      <th>39997</th>\n",
       "      <th>39998</th>\n",
       "      <th>39999</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "      <th>uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.086275</td>\n",
       "      <td>0.101961</td>\n",
       "      <td>0.101961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>Blended</td>\n",
       "      <td>0</td>\n",
       "      <td>8821c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.439216</td>\n",
       "      <td>0.478431</td>\n",
       "      <td>0.572549</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.749020</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.760784</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>Denim</td>\n",
       "      <td>1</td>\n",
       "      <td>1503c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>0.298039</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168627</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.125490</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.160784</td>\n",
       "      <td>0.192157</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>Polyester</td>\n",
       "      <td>2</td>\n",
       "      <td>16132c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.380392</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.356863</td>\n",
       "      <td>0.345098</td>\n",
       "      <td>0.341176</td>\n",
       "      <td>0.345098</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.423529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.415686</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.545098</td>\n",
       "      <td>Blended</td>\n",
       "      <td>0</td>\n",
       "      <td>3621d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.298039</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.298039</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.278431</td>\n",
       "      <td>0.345098</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>3</td>\n",
       "      <td>2333a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40003 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.062745  0.058824  0.066667  0.062745  0.062745  0.066667  0.082353   \n",
       "1  0.266667  0.258824  0.270588  0.301961  0.313725  0.352941  0.439216   \n",
       "2  0.235294  0.247059  0.274510  0.298039  0.309804  0.305882  0.282353   \n",
       "3  0.380392  0.364706  0.356863  0.345098  0.341176  0.345098  0.388235   \n",
       "4  0.321569  0.317647  0.305882  0.298039  0.294118  0.301961  0.309804   \n",
       "\n",
       "          7         8         9  ...     39993     39994     39995     39996  \\\n",
       "0  0.086275  0.101961  0.101961  ...  0.066667  0.058824  0.054902  0.054902   \n",
       "1  0.478431  0.572549  0.607843  ...  0.200000  0.333333  0.749020  0.803922   \n",
       "2  0.274510  0.270588  0.258824  ...  0.168627  0.133333  0.125490  0.133333   \n",
       "3  0.435294  0.450980  0.423529  ...  0.400000  0.415686  0.450980  0.482353   \n",
       "4  0.313725  0.325490  0.333333  ...  0.376471  0.298039  0.247059  0.278431   \n",
       "\n",
       "      39997     39998     39999   category  label     uid  \n",
       "0  0.058824  0.062745  0.062745    Blended      0   8821c  \n",
       "1  0.788235  0.760784  0.717647      Denim      1   1503c  \n",
       "2  0.160784  0.192157  0.211765  Polyester      2  16132c  \n",
       "3  0.482353  0.505882  0.545098    Blended      0   3621d  \n",
       "4  0.345098  0.333333  0.309804     Cotton      3   2333a  \n",
       "\n",
       "[5 rows x 40003 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = './Subsamples/train'\n",
    "X, Y, unique_ids = parse_data(folder_path)\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df['category'] = pd.Categorical(Y)\n",
    "df['label'], _ = pd.factorize(df['category'])\n",
    "df['uid'] = unique_ids\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this dataframe, which is in the same format as the data shown in the MNIST D200 homework, it should be easy to set up t-SNE and PCA. HOWEVER - at this point we don't have any true features, as our existing features need to be converted to columns in this dataframe. Below is skeleton code that can be filled out to run and add these features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO - INSERT PCA AND OTHER EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>39990</th>\n",
       "      <th>39991</th>\n",
       "      <th>39992</th>\n",
       "      <th>39993</th>\n",
       "      <th>39994</th>\n",
       "      <th>39995</th>\n",
       "      <th>39996</th>\n",
       "      <th>39997</th>\n",
       "      <th>39998</th>\n",
       "      <th>39999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Blended</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.086275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086275</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.062745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Denim</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.439216</td>\n",
       "      <td>0.478431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.149020</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.749020</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.760784</td>\n",
       "      <td>0.717647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Polyester</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>0.298039</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160784</td>\n",
       "      <td>0.180392</td>\n",
       "      <td>0.160784</td>\n",
       "      <td>0.168627</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.125490</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.160784</td>\n",
       "      <td>0.192157</td>\n",
       "      <td>0.211765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Blended</td>\n",
       "      <td>0.380392</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.356863</td>\n",
       "      <td>0.345098</td>\n",
       "      <td>0.341176</td>\n",
       "      <td>0.345098</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.415686</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.545098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.298039</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.309804</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.576471</td>\n",
       "      <td>0.462745</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.298039</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.278431</td>\n",
       "      <td>0.345098</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.309804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label   category         0         1         2         3         4  \\\n",
       "0      0    Blended  0.062745  0.058824  0.066667  0.062745  0.062745   \n",
       "1      1      Denim  0.266667  0.258824  0.270588  0.301961  0.313725   \n",
       "2      2  Polyester  0.235294  0.247059  0.274510  0.298039  0.309804   \n",
       "3      0    Blended  0.380392  0.364706  0.356863  0.345098  0.341176   \n",
       "4      3     Cotton  0.321569  0.317647  0.305882  0.298039  0.294118   \n",
       "\n",
       "          5         6         7  ...     39990     39991     39992     39993  \\\n",
       "0  0.066667  0.082353  0.086275  ...  0.086275  0.082353  0.070588  0.066667   \n",
       "1  0.352941  0.439216  0.478431  ...  0.164706  0.149020  0.176471  0.200000   \n",
       "2  0.305882  0.282353  0.274510  ...  0.160784  0.180392  0.160784  0.168627   \n",
       "3  0.345098  0.388235  0.435294  ...  0.270588  0.270588  0.305882  0.400000   \n",
       "4  0.301961  0.309804  0.313725  ...  0.576471  0.462745  0.411765  0.376471   \n",
       "\n",
       "      39994     39995     39996     39997     39998     39999  \n",
       "0  0.058824  0.054902  0.054902  0.058824  0.062745  0.062745  \n",
       "1  0.333333  0.749020  0.803922  0.788235  0.760784  0.717647  \n",
       "2  0.133333  0.125490  0.133333  0.160784  0.192157  0.211765  \n",
       "3  0.415686  0.450980  0.482353  0.482353  0.505882  0.545098  \n",
       "4  0.298039  0.247059  0.278431  0.345098  0.333333  0.309804  \n",
       "\n",
       "[5 rows x 40002 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reorder \n",
    "train_df = df\n",
    "train_df.columns = train_df.columns.astype(str)\n",
    "PIXEL_COLS = train_df.columns.tolist()[:-3] # list of pixel header\n",
    "LABEL_COLS = ['label', 'category'] # list of labels header\n",
    "\n",
    "cols_reorder = LABEL_COLS + PIXEL_COLS\n",
    "train_df = train_df[cols_reorder]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose  a sample of the data to see if works:\n",
    "# Sample data\n",
    "sampled_train_df = train_df.sample(frac=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>39990</th>\n",
       "      <th>39991</th>\n",
       "      <th>39992</th>\n",
       "      <th>39993</th>\n",
       "      <th>39994</th>\n",
       "      <th>39995</th>\n",
       "      <th>39996</th>\n",
       "      <th>39997</th>\n",
       "      <th>39998</th>\n",
       "      <th>39999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>2</td>\n",
       "      <td>Polyester</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.086275</td>\n",
       "      <td>0.094118</td>\n",
       "      <td>0.109804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.160784</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.145098</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>0.109804</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.070588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18431</th>\n",
       "      <td>1</td>\n",
       "      <td>Denim</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.262745</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>0.262745</td>\n",
       "      <td>0.262745</td>\n",
       "      <td>0.278431</td>\n",
       "      <td>0.250980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250980</td>\n",
       "      <td>0.239216</td>\n",
       "      <td>0.219608</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250980</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.286275</td>\n",
       "      <td>0.258824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2311</th>\n",
       "      <td>3</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.203922</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.227451</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.250980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.298039</td>\n",
       "      <td>0.243137</td>\n",
       "      <td>0.223529</td>\n",
       "      <td>0.223529</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.384314</td>\n",
       "      <td>0.396078</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>0</td>\n",
       "      <td>Blended</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.443137</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.360784</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.368627</td>\n",
       "      <td>0.368627</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.356863</td>\n",
       "      <td>0.341176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>1</td>\n",
       "      <td>Denim</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.094118</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.074510</td>\n",
       "      <td>0.121569</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.239216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18067</th>\n",
       "      <td>3</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>0.223529</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.231373</td>\n",
       "      <td>0.262745</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.384314</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.749020</td>\n",
       "      <td>0.756863</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.796078</td>\n",
       "      <td>0.831373</td>\n",
       "      <td>0.843137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3842</th>\n",
       "      <td>0</td>\n",
       "      <td>Blended</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.160784</td>\n",
       "      <td>0.160784</td>\n",
       "      <td>0.168627</td>\n",
       "      <td>0.168627</td>\n",
       "      <td>0.168627</td>\n",
       "      <td>0.192157</td>\n",
       "      <td>0.243137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145098</td>\n",
       "      <td>0.145098</td>\n",
       "      <td>0.172549</td>\n",
       "      <td>0.192157</td>\n",
       "      <td>0.192157</td>\n",
       "      <td>0.203922</td>\n",
       "      <td>0.192157</td>\n",
       "      <td>0.184314</td>\n",
       "      <td>0.160784</td>\n",
       "      <td>0.141176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>3</td>\n",
       "      <td>Cotton</td>\n",
       "      <td>0.713725</td>\n",
       "      <td>0.701961</td>\n",
       "      <td>0.701961</td>\n",
       "      <td>0.694118</td>\n",
       "      <td>0.698039</td>\n",
       "      <td>0.709804</td>\n",
       "      <td>0.721569</td>\n",
       "      <td>0.741176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709804</td>\n",
       "      <td>0.709804</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.737255</td>\n",
       "      <td>0.749020</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.756863</td>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10331</th>\n",
       "      <td>2</td>\n",
       "      <td>Polyester</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.298039</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.223529</td>\n",
       "      <td>0.192157</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278431</td>\n",
       "      <td>0.341176</td>\n",
       "      <td>0.415686</td>\n",
       "      <td>0.439216</td>\n",
       "      <td>0.396078</td>\n",
       "      <td>0.356863</td>\n",
       "      <td>0.349020</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.313725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12490</th>\n",
       "      <td>4</td>\n",
       "      <td>Wool</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.145098</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.145098</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>0.109804</td>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.192157</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.109804</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.070588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4426 rows × 40002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label   category         0         1         2         3         4  \\\n",
       "2708       2  Polyester  0.054902  0.050980  0.054902  0.070588  0.078431   \n",
       "18431      1      Denim  0.235294  0.262745  0.274510  0.270588  0.262745   \n",
       "2311       3     Cotton  0.200000  0.203922  0.215686  0.227451  0.235294   \n",
       "1377       0    Blended  0.313725  0.305882  0.301961  0.305882  0.337255   \n",
       "336        1      Denim  0.078431  0.094118  0.082353  0.050980  0.047059   \n",
       "...      ...        ...       ...       ...       ...       ...       ...   \n",
       "18067      3     Cotton  0.223529  0.215686  0.211765  0.231373  0.262745   \n",
       "3842       0    Blended  0.156863  0.160784  0.160784  0.168627  0.168627   \n",
       "2222       3     Cotton  0.713725  0.701961  0.701961  0.694118  0.698039   \n",
       "10331      2  Polyester  0.301961  0.298039  0.266667  0.235294  0.223529   \n",
       "12490      4       Wool  0.141176  0.145098  0.133333  0.156863  0.145098   \n",
       "\n",
       "              5         6         7  ...     39990     39991     39992  \\\n",
       "2708   0.086275  0.094118  0.109804  ...  0.164706  0.160784  0.156863   \n",
       "18431  0.262745  0.278431  0.250980  ...  0.250980  0.239216  0.219608   \n",
       "2311   0.247059  0.247059  0.250980  ...  0.325490  0.298039  0.243137   \n",
       "1377   0.364706  0.443137  0.447059  ...  0.352941  0.352941  0.360784   \n",
       "336    0.047059  0.047059  0.050980  ...  0.078431  0.070588  0.058824   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "18067  0.325490  0.384314  0.388235  ...  0.717647  0.729412  0.733333   \n",
       "3842   0.168627  0.192157  0.243137  ...  0.145098  0.145098  0.172549   \n",
       "2222   0.709804  0.721569  0.741176  ...  0.709804  0.709804  0.717647   \n",
       "10331  0.192157  0.176471  0.176471  ...  0.278431  0.341176  0.415686   \n",
       "12490  0.164706  0.137255  0.129412  ...  0.050980  0.050980  0.109804   \n",
       "\n",
       "          39993     39994     39995     39996     39997     39998     39999  \n",
       "2708   0.145098  0.141176  0.129412  0.109804  0.098039  0.082353  0.070588  \n",
       "18431  0.188235  0.200000  0.250980  0.301961  0.301961  0.286275  0.258824  \n",
       "2311   0.223529  0.223529  0.337255  0.384314  0.396078  0.376471  0.352941  \n",
       "1377   0.364706  0.368627  0.368627  0.364706  0.364706  0.356863  0.341176  \n",
       "336    0.054902  0.054902  0.058824  0.074510  0.121569  0.196078  0.239216  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "18067  0.733333  0.749020  0.756863  0.764706  0.796078  0.831373  0.843137  \n",
       "3842   0.192157  0.192157  0.203922  0.192157  0.184314  0.160784  0.141176  \n",
       "2222   0.725490  0.737255  0.749020  0.752941  0.756863  0.772549  0.800000  \n",
       "10331  0.439216  0.396078  0.356863  0.349020  0.337255  0.325490  0.313725  \n",
       "12490  0.152941  0.215686  0.192157  0.176471  0.109804  0.070588  0.070588  \n",
       "\n",
       "[4426 rows x 40002 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA (From data 200 code, lecture-26) - This is too heavy to run\n",
    "n_comps = 50\n",
    "PCA_COLS = [f\"pc{i+1}\" for i in range(n_comps)]\n",
    "pca = PCA(n_components=n_comps)\n",
    "pca.fit(sampled_train_df[PIXEL_COLS])\n",
    "principal_components = pca.transform(sampled_train_df[PIXEL_COLS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8897120776964395"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8465544761266028"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PC1, PC2 component scores\n",
    "np.sum(pca.explained_variance_ratio_[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "variances = pca.explained_variance_ratio_\n",
    "\n",
    "cumulatives = [] \n",
    "\n",
    "for i in range(len(variances)):\n",
    "    if i == 0:\n",
    "        cumulative = variances[i]  # For the first element, cumulative sum is the variance itself\n",
    "    else:\n",
    "        cumulative += variances[i]  # Accumulate the current variance to the previous cumulative sum\n",
    "    cumulatives.append(cumulative)\n",
    "\n",
    "cumulatives\n",
    "# plot the explained variance (here we have to do it in the opposite side)\n",
    "plt.plot(np.arange(n_comps)+1,\n",
    "         cumulatives,\n",
    "         marker='.');\n",
    "plt.ylabel(\"% variance\")\n",
    "plt.xlabel(\"Component Number\")\n",
    "plt.title(\"Variance Explained by each PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - Multi-Class Classifier\n",
    "\n",
    "\n",
    "The function below takes in a test DF, features (which we can modified in the event we have thousands of columns and they are named numerically -- in this case we just need to drop the target variable to define X). It runs cross validation to determine the best hyperparameters to be used for the support vector machine model, returning a grid search object with the following attributes (per ChatGPT):\n",
    "\n",
    "<b>Attributes</b>\n",
    "* best_estimator_: The estimator that was chosen by the search, i.e., the estimator which gave highest score (or smallest loss if specified) on the left out data. Not available if refit=False.\n",
    "* best_score_: The score of the best_estimator on the left out data.\n",
    "* best_params_: The parameter setting that gave the best results on the hold out data.\n",
    "* best_index_: The index (of the cv_results_ arrays) which corresponds to the best candidate parameter setting.\n",
    "* cv_results_: A dictionary with keys as column headers and values as columns, that can be imported into a pandas DataFrame. This attribute provides scores, fit times, score times, and parameters for all the candidate models. It contains a lot of detailed information for each parameter combination that was evaluated.\n",
    "* scorer_: The function or a dictionary of functions that scores the predictions on the test set.\n",
    "* n_splits_: The number of cross-validation splits (folds/iterations).\n",
    "* refit_time_: Time for refitting the best estimator on the whole dataset (available only if refit is set to True).\n",
    "\n",
    "<b>Methods</b>\n",
    "* fit(X, y=None, groups=None): Run fit with all sets of parameters.\n",
    "* predict(X): Call predict on the estimator with the best found parameters.\n",
    "* score(X, y=None): Returns the score on the given data, if the estimator has been refit.\n",
    "* predict_proba(X): Call predict_proba on the estimator with the best found parameters, if available.\n",
    "* decision_function(X): Call decision_function on the estimator with the best found parameters, if available.\n",
    "* transform(X): Call transform on the estimator with the best found parameters, if available.\n",
    "* inverse_transform(X): Call inverse_transform on the estimator with the best found parameters, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_grid_search_cv(dataframe, features, target, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Perform grid search cross-validation for SVM classifier on the given dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe: The pandas DataFrame containing the dataset.\n",
    "    - features: List of column names to be used as features.\n",
    "    - target: The name of the column to be used as the target variable.\n",
    "    - cv_folds: Number of folds for cross-validation.\n",
    "    \n",
    "    Returns:\n",
    "    - grid_search: The fitted GridSearchCV object.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate the features and the target variable\n",
    "    X = dataframe[features]\n",
    "    y = dataframe[target]\n",
    "    \n",
    "    # Split the data into training and testing sets (optional, could also perform CV on the entire dataset)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Define a pipeline that includes scaling and the classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Feature scaling is important for SVM\n",
    "        ('svm', SVC(probability=True))  # SVM classifier\n",
    "    ])\n",
    "    \n",
    "    # Parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'svm__C': [0.1, 1, 10],  # SVM regularization parameter\n",
    "        'svm__kernel': ['linear', 'rbf'],  # Kernel type to be used in the algorithm\n",
    "        'svm__gamma': ['scale', 'auto']  # Kernel coefficient for 'rbf', 'poly' and 'sigmoid'\n",
    "    }\n",
    "    \n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=cv_folds, scoring='accuracy', verbose=1)\n",
    "    \n",
    "    # Perform grid search cross-validation\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best parameters found: \", grid_search.best_params_)\n",
    "    print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "    \n",
    "    # Optionally, evaluate on the test set\n",
    "    test_score = grid_search.score(X_test, y_test)\n",
    "    print(\"Test set score: {:.2f}\".format(test_score))\n",
    "    \n",
    "    return grid_search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "The below code has not been tested or debugged - it was read through against a TDS article for accuracy, but it would need significant work, change in approach (and potentially additional compute) if we are to make it work. I wanted to store it so I don't lose the information though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple cnn model\n",
    "# def create_cnn_model(input_shape, num_classes):\n",
    "#     model = models.Sequential([\n",
    "#         # convolutional layer with ReLU activation and Max Pooling\n",
    "#         layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "#         layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "#         # convolutional layer 2\n",
    "#         layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#         layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "#         # flatten the output and add dense layers for classification\n",
    "#         layers.Flatten(),\n",
    "#         layers.Dense(64, activation='relu'),\n",
    "#         layers.Dense(num_classes, activation='softmax')\n",
    "#     ])\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# input_shape = (64, 64, 3)  # input shape (height, width, channels)\n",
    "# num_classes = 5\n",
    "\n",
    "# model = create_cnn_model(input_shape, num_classes)\n",
    "\n",
    "# # compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# # train_images, train_labels = ... \n",
    "# # val_images, val_labels = ... \n",
    "\n",
    "# train model\n",
    "# history = model.fit(train_images, train_labels, epochs=10, validation_data=(val_images, val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO - ADD SVM CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
