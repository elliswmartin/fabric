{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base data sci libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# image processing libraries\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(23)\n",
    "plt.rcParams['figure.figsize'] = (4, 4)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "from utils.feature_utils import create_visual_vocab, extract_bovw_features, extract_hog_features, extract_wavelet_features, extract_log_features, extract_normals_features, extract_gabor_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To-Dos\n",
    "* Finalize feature selections & tune features\n",
    "* Once we have features, tweak and play with the grid-search CV to get some baseline results for our model\n",
    "* tSNE\n",
    "* Update Pickle once tune features ? \n",
    "\n",
    "\n",
    "### Done\n",
    "* Add feature functions into cell with extract_log_features() mimicking the format and the instructions there\n",
    "* Update parse_data() to include any features generated from the functions defined in the featurize block\n",
    "* Add PCA and any other data exploration we want to complete for feature selection\n",
    "* Decide on strategy in terms of flattening images or using scalar features -- what is worth spending computational umph on based on data exploration? - NO ?\n",
    "* Run PCA on existing features \n",
    "* Pickle the feature df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below parser to produce the pickle file, which parses the raw image data and generates several arrays. These arrays are then turned into a DF and sent to a pickle file down below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Data to DF + Add In Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_all(folder_path):\n",
    "    \"\"\" \n",
    "    Run all feature functions and create df of all feature representations. Scalar or Vector \n",
    "    \n",
    "    Returns: \n",
    "        X (np.array): Feature values \n",
    "        Y (np.array): Categorical label for each image  \n",
    "        unique_ids (np.array): image IDs  \n",
    "    \"\"\"\n",
    "\n",
    "    image_vectors = []  # image data\n",
    "    labels = []  # labels\n",
    "    ids = []  # unique IDs\n",
    "    features = []\n",
    "\n",
    "    kmeans_fitted = create_visual_vocab(folder_path)\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".png\"):\n",
    "            parts = filename.split('_')\n",
    "            fabType = parts[0]\n",
    "            id1 = parts[1]\n",
    "            id2 = parts[3].split('.')[0]  # Remove .png extension\n",
    "            \n",
    "            unique_id = id1 + id2\n",
    "\n",
    "            image = Image.open(os.path.join(folder_path, filename)).convert('L')\n",
    "            img_array = np.array(image)\n",
    "\n",
    "            # sift expects a particular type of image so this needs to be done before \n",
    "            # the image is normalized for the other features. The output vector is between\n",
    "            # 0 and 1 and should not impact PCA\n",
    "            bovw_feature_vector = extract_bovw_features(img_array, kmeans_fitted)\n",
    "            \n",
    "            # normalize the image vector to be between 0 and 1 \n",
    "            img_array_std = (img_array - np.mean(img_array)) / np.std(img_array)\n",
    "\n",
    "            ### SCALAR FEATURES ###\n",
    "            hog_stats = extract_hog_features(img_array_std, 4, 20)\n",
    "            hog_mean, hog_sum, hog_var, hog_skew, hog_kurt = hog_stats[:5]\n",
    "            wavelet_stats = extract_wavelet_features(img_array_std)\n",
    "            wave_mean_cA, wave_var_cA, wave_mean_cD, wave_var_cD = wavelet_stats[:4]\n",
    "            # bovw_features = extract_bovw_features(img_array_std, 5, False)\n",
    "            # bovw_1, bovw_2, bovw_3, bovw_4, bovw_5 = bovw_features[:5] \n",
    "            \n",
    "            scalar_features = []\n",
    "            scalar_features.extend([\n",
    "                extract_log_features(img_array_std), \n",
    "                extract_normals_features(img_array_std), \n",
    "                extract_gabor_features(img_array_std),\n",
    "                hog_mean, hog_sum, hog_var, hog_skew, hog_kurt, \n",
    "                wave_mean_cA, wave_var_cA, wave_mean_cD, wave_var_cD, \n",
    "                *bovw_feature_vector])\n",
    "        \n",
    "            scalar_features_array = np.array(scalar_features)\n",
    "\n",
    "            \"\"\"\n",
    "            \n",
    "            ### VECTORIZED FEATURES ###\n",
    "            # new_vector_with_scalar = np.append(img_vector_normalized, log_scalar)\n",
    "\n",
    "            # log_vector = extract_log_features(img_array, scalar=False)\n",
    "            # hog_vector = extract_hog_features(img_array, 4, 20, scalar=False)\n",
    "            # normals_vector = extract_normals_features(img_array, scalar = False)\n",
    "            \n",
    "            #NOTE: This is created but then not used, I added as an additional output (features)  \n",
    "            # final_img_feature_vector  = np.concatenate((img_vector_normalized, scalar_features_array, log_vector, hog_vector, normals_vector)) # BE SURE TO ADD ANY FEATURE VECTORS HERE\n",
    "\n",
    "            # final_img_feature_vector  = np.concatenate((img_vector_normalized, scalar_features_array)) # flattened image vector + scalar features\n",
    "            # image_vectors.append(img_vector_normalized)\n",
    "            # features.append(final_img_feature_vector)\n",
    "                 \n",
    "            \"\"\"\n",
    "\n",
    "            image_vectors.append(scalar_features_array)\n",
    "            labels.append(fabType)\n",
    "            ids.append(unique_id)\n",
    "\n",
    "    X = np.array(image_vectors)\n",
    "    Y = np.array(labels)\n",
    "    unique_ids = np.array(ids)\n",
    "    # features_array = np.array(features)\n",
    "    # return X, Y, unique_ids, features_array\n",
    "    return X, Y, unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driver for PARSE ALL \n",
    "folder_path = './Subsamples/train'\n",
    "# X, Y, unique_ids, features_array = parse_all(folder_path)\n",
    "X, Y, unique_ids = parse_all(folder_path)\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df['category'] = pd.Categorical(Y)\n",
    "df['label'], _ = pd.factorize(df['category'])\n",
    "df['uid'] = unique_ids\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¥’ PICKLEðŸš°ðŸ“ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pickle the dataframe\n",
    "train_df = df.copy()\n",
    "train_df.columns = train_df.columns.astype(str)\n",
    "PIXEL_COLS = train_df.columns.tolist()[:-3] # list of pixel header\n",
    "LABEL_COLS = ['label', 'category'] # list of labels header\n",
    "cols_reorder = LABEL_COLS + PIXEL_COLS\n",
    "train_df = train_df[cols_reorder]\n",
    "train_df.head()\n",
    "\n",
    "curr_date = '04dd' # Replace dd with date \n",
    "filename = f'./Subsamples/train_{curr_date}.pkl'\n",
    "train_df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22128, 34)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the pickled df  \n",
    "filename = './pkls/train_0406.pkl'\n",
    "train_df = pd.read_pickle(filename)\n",
    "train_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
